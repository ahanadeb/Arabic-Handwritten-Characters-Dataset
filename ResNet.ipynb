{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 20\n",
    "num_classes = 28\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataset(Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        labels = self.landmarks_frame\n",
    "        labels = np.array(labels)\n",
    "        labels = labels.astype('float')\n",
    "        \n",
    "        A=[1]\n",
    "        labels = np.vstack([A, labels])\n",
    "        \n",
    "        \n",
    "        #print(labels.shape[0])\n",
    "        #print(labels)\n",
    "        get_label=labels[idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        labels=get_label.astype('float');\n",
    "        \n",
    "        \n",
    "        str1= 'id_'+str(idx+1)+'_label_'+str(int(labels))+'.png';\n",
    "        labels=labels-1;\n",
    "        img_name = os.path.join(self.root_dir, str1)\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        sample = {'image': image, 'labels': labels}\n",
    "        #print(\"idx\", \" \", idx, \" \", \"labels\", labels)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, labels = sample['image'], sample['labels']\n",
    "        #print(type(image))\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        #image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'labels': torch.from_numpy(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = GetDataset(csv_file='arabic_letters/csvTrainLabel 13440x1.csv',\n",
    "                                           root_dir='arabic_letters/Train Images 13440x32x32/train',\n",
    "                                           transform=transforms.Compose([\n",
    "                                              \n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GetDataset(csv_file='arabic_letters/csvTestLabel 3360x1.csv',\n",
    "                                           root_dir='arabic_letters/Test Images 3360x32x32/test',\n",
    "                                           transform=transforms.Compose([\n",
    "                                              \n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot(epochs, loss):\n",
    "    plt.plot(epochs, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=28):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(1, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/135] Loss: 1.0102\n",
      "Epoch [2/20], Step [100/135] Loss: 0.4414\n",
      "Epoch [3/20], Step [100/135] Loss: 0.2471\n",
      "Epoch [4/20], Step [100/135] Loss: 0.1875\n",
      "Epoch [5/20], Step [100/135] Loss: 0.1765\n",
      "Epoch [6/20], Step [100/135] Loss: 0.1968\n",
      "Epoch [7/20], Step [100/135] Loss: 0.1248\n",
      "Epoch [8/20], Step [100/135] Loss: 0.1904\n",
      "Epoch [9/20], Step [100/135] Loss: 0.1215\n",
      "Epoch [10/20], Step [100/135] Loss: 0.0682\n",
      "Epoch [11/20], Step [100/135] Loss: 0.0557\n",
      "Epoch [12/20], Step [100/135] Loss: 0.0622\n",
      "Epoch [13/20], Step [100/135] Loss: 0.0829\n",
      "Epoch [14/20], Step [100/135] Loss: 0.0458\n",
      "Epoch [15/20], Step [100/135] Loss: 0.0630\n",
      "Epoch [16/20], Step [100/135] Loss: 0.0389\n",
      "Epoch [17/20], Step [100/135] Loss: 0.0763\n",
      "Epoch [18/20], Step [100/135] Loss: 0.0213\n",
      "Epoch [19/20], Step [100/135] Loss: 0.1477\n",
      "Epoch [20/20], Step [100/135] Loss: 0.0637\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfpUlEQVR4nO3dfXAcd53n8fd3ZqQZSZZGtiVZkh9iAyHBITGX1fqAsCHhIeuEhxwPtxsXxcMeu67ckaqFveM2FFsJt1u1dcAu7AGBbA6yWa6WBHZJwHuVB+cWWAe4gOWck9hJnBjHSRTZlmzHliVbDzPzvT+6pYzlGWtsjTRy9+dVNdUz3b+e+U17/OnWr3/dP3N3REQkuhK1roCIiMwtBb2ISMQp6EVEIk5BLyIScQp6EZGIU9CLiERcaqYCZnYn8F5gwN3fWGL5Z4GPFL3fG4B2dz9iZvuA40AeyLl7TyWVamtr89WrV1f0BUREBLZv337I3dtLLbOZ+tGb2ZXAMPDdUkE/rez7gM+4+zvC1/uAHnc/dDYV7unp8d7e3rNZRUQk1sxse7mD6Rmbbtx9K3Ckws/aCNx9FnUTEZE5VrU2ejNrBDYAPyya7cAWM9tuZpuq9VkiIlK5Gdvoz8L7gF+4e/HR/xXu3m9mHcDDZvZM+BfCacIdwSaAVatWVbFaIiLxVs1eNzcwrdnG3fvD6QBwH7C+3Mrufoe797h7T3t7yfMJIiJyDqoS9GaWBd4O/LhoXpOZNU8+B64Bdlbj80REpHKVdK+8G7gKaDOzPuBWoA7A3W8Pi30A2OLuI0WrLgPuM7PJz/meuz9YvaqLiEglZgx6d99YQZm7gLumzdsLrDvXiomISHVE5srYQsH5xk+eY+uzg7WuiojIghKZoE8kjL/dupd/efpgrasiIrKgRCboAbqzDfQfG611NUREFpRIBX1Xa4YDCnoRkVNEK+izDew/drLW1RARWVAiFvQZDg2PM5bL17oqIiILRuSCHlDzjYhIkUgFfXdrAwD7FfQiIlMiFfSd4RG92ulFRF4VqaDvzgZH9P1HdUQvIjIpUkHfUJ+ktbFObfQiIkUiFfSgLpYiItNFMOgzaroRESkSyaA/MKSgFxGZFLmg725t4MjIOKMTumhKRAQiGPSdLZNdLHVULyICEQz6rtYw6I/qhKyICEQw6Cf70uuIXkQkELmg19WxIiKnilzQZ+qSLGmq1wAkIiKhyAU9hF0sFfQiIkAFQW9md5rZgJntLLP8KjM7ZmY7wsctRcs2mNluM9tjZjdXs+Jn0pVtoF8nY0VEgMqO6O8CNsxQ5hF3f1P4+HMAM0sCtwHXAmuBjWa2djaVrVRXNqOTsSIioRmD3t23AkfO4b3XA3vcfa+7jwP3ANefw/ucta7WDMdOTnBiPDcfHycisqBVq43+LWb2uJk9YGaXhPOWAy8VlekL5805dbEUEXlVNYL+MeACd18HfB34UTjfSpT1cm9iZpvMrNfMegcHB2dVoakulrq5mYjI7IPe3YfcfTh8fj9QZ2ZtBEfwK4uKrgD6z/A+d7h7j7v3tLe3z6pOUwOQqC+9iMjsg97MOs3Mwufrw/c8DGwDLjSzNWZWD9wAbJ7t51ViWTYNaJBwERGA1EwFzOxu4Cqgzcz6gFuBOgB3vx34MPAfzSwHnARucHcHcmZ2E/AQkATudPddc/ItpkmnkrQtqtfVsSIiVBD07r5xhuXfAL5RZtn9wP3nVrXZCfrS64heRCSSV8aCro4VEZkU2aDvbm3QyVgRESIc9J3ZDMdHcwyP6aIpEYm3yAZ9V1YDkIiIQISDvrtVV8eKiECEg/7VsWN1RC8i8RbdoM9mMENdLEUk9iIb9HXJBO2L0upiKSKxF9mgh+CErLpYikjcRTzoG3QyVkRiL9pB36qrY0VEIh303dkGhsdyDI1O1LoqIiI1E+mg1wAkIiIRD/ru1iDodUJWROIs0kHfFY40pXZ6EYmzSAd9R3OahOl+NyISb5EO+lQyQUdzhn4d0YtIjEU66EFdLEVEoh/0ujpWRGIuBkHfwP6jowTjlYuIxE8Mgj7DyYk8x07qoikRiacZg97M7jSzATPbWWb5R8zsifDxSzNbV7Rsn5k9aWY7zKy3mhWvlAYgEZG4q+SI/i5gwxmWPw+83d0vA/4CuGPa8qvd/U3u3nNuVZydqatj1U4vIjGVmqmAu281s9VnWP7LopePAiuqUK+q6Q4vmtIAJCISV9Vuo/8k8EDRawe2mNl2M9tU5c+qSHtzmmTC1MVSRGJrxiP6SpnZ1QRB/7ai2Ve4e7+ZdQAPm9kz7r61zPqbgE0Aq1atqla1SCaMZc1pdbEUkdiqyhG9mV0GfBu43t0PT8539/5wOgDcB6wv9x7ufoe797h7T3t7ezWqNaWrtUF3sBSR2Jp10JvZKuBe4KPu/mzR/CYza558DlwDlOy5M9e6shkODCnoRSSeZmy6MbO7gauANjPrA24F6gDc/XbgFmAp8E0zA8iFPWyWAfeF81LA99z9wTn4DjPqbm3g4acO4u6E9RERiY1Ket1snGH5HwJ/WGL+XmDd6WvMv86WDGO5Aq+cmGBJU32tqyMiMq8if2UsFA1AotsVi0gMxSLoNQCJiMRZTIJeV8eKSHzFIujbFqWpS5oGIBGRWIpF0CcSxrIWDUAiIvEUi6CHcAASnYwVkRiKUdA36FbFIhJL8Qn6cOxYjTQlInETm6DvzjYwni9weGS81lUREZlXsQn6qQFIdHMzEYmZ2AT91AAk6ksvIjETm6DvCm+DoC6WIhI3sQn6JY311CcTOqIXkdiJTdAnEkZnNqM2ehGJndgEPYQDkKjpRkRiJnZBr6YbEYmbeAV9awMHh0YpFHTRlIjER6yCvjubYSLvHBoeq3VVRETmTayCfnIAEt3zRkTiJFZB36kBSEQkhmIV9N2t4dWx6mIpIjEyY9Cb2Z1mNmBmO8ssNzP7mpntMbMnzOzyomUbzGx3uOzmalb8XCxurCOdSnBgSEEvIvFRyRH9XcCGMyy/FrgwfGwCvgVgZkngtnD5WmCjma2dTWVny8w0AImIxM6MQe/uW4EjZyhyPfBdDzwKtJpZF7Ae2OPue919HLgnLFtTGoBEROKmGm30y4GXil73hfPKza+pyQFIRETiohpBbyXm+Rnml34Ts01m1mtmvYODg1WoVmld2QwHhkbJ66IpEYmJagR9H7Cy6PUKoP8M80ty9zvcvcfde9rb26tQrdK6sg3kC87gcV00JSLxUI2g3wx8LOx982bgmLvvB7YBF5rZGjOrB24Iy9ZUd3hfet3zRkTiIjVTATO7G7gKaDOzPuBWoA7A3W8H7geuA/YAJ4A/CJflzOwm4CEgCdzp7rvm4DuclcmrY9VOLyJxMWPQu/vGGZY78Kkyy+4n2BEsGF3h1bHqYikicRGrK2MBsg11NNQl1cVSRGIjdkFvZupiKSKxErugBw1AIiLxEtOgb9DYsSISG7EM+u5shoHjo+TyhVpXRURkzsUy6DuzDRQcBnTRlIjEQCyDvqtVA5CISHzEMui7sxqARETiI5ZBP3lEry6WIhIHsQz65nSKpvqkuliKSCzEMuiDi6bUxVJE4iGWQQ/BRVP7NXasiMRAvINeNzYTkRiIcdA3MDg8xnhOF02JSLTFNui7WzO4w0E134hIxMU26KcGIFHQi0jExTjoNQCJiMRDfIO+NTii1wAkIhJ1sQ36RekUzZmUro4VkciLbdBDOACJmm5EJOJiHvQNaroRkcirKOjNbIOZ7TazPWZ2c4nlnzWzHeFjp5nlzWxJuGyfmT0ZLuut9heYje7WjIJeRCIvNVMBM0sCtwHvBvqAbWa22d2fmizj7l8GvhyWfx/wGXc/UvQ2V7v7oarWvAo6Wxo4NDzGWC5POpWsdXVEROZEJUf064E97r7X3ceBe4Drz1B+I3B3NSo31yZvV3zwmEaaEpHoqiTolwMvFb3uC+edxswagQ3AD4tmO7DFzLab2aZzrehcmBqARLcrFpEIm7HpBrAS87xM2fcBv5jWbHOFu/ebWQfwsJk94+5bT/uQYCewCWDVqlUVVGv2NACJiMRBJUf0fcDKotcrgP4yZW9gWrONu/eH0wHgPoKmoNO4+x3u3uPuPe3t7RVUa/amro7VEb2IRFglQb8NuNDM1phZPUGYb55eyMyywNuBHxfNazKz5snnwDXAzmpUvBoa61NkG+o0AImIRNqMTTfunjOzm4CHgCRwp7vvMrMbw+W3h0U/AGxx95Gi1ZcB95nZ5Gd9z90frOYXmK2urLpYiki0VdJGj7vfD9w/bd7t017fBdw1bd5eYN2sajjHgqBX042IRFesr4yF4OZmOqIXkSiLfdB3ZzMcGRlndCJf66qIiMyJ2Ad95+QAJDqqF5GIin3Qd6uLpYhEXOyDfmoAEnWxFJGIUtCHR/QaO1ZEoir2QZ+pS7K4sU4DkIhIZMU+6EEDkIhItCno0QAkIhJtCnqgU1fHikiEKegJmm6Onpjg5LgumhKR6FHQEzTdgPrSi0g0KegJxo4FXR0rItGkoKfoiF5dLEUkghT0BCdjAfW8EZFIUtAD6VSStkX1CnoRiSQFfUhdLEUkqhT0oa5sg25sJiKRpKAPdeuIXkQiSkEf6sw2MDSaY2QsV+uqiIhUVUVBb2YbzGy3me0xs5tLLL/KzI6Z2Y7wcUul6y4Uk10sdVQvIlGTmqmAmSWB24B3A33ANjPb7O5PTSv6iLu/9xzXrbmucEjB/qOjvK6juca1ERGpnkqO6NcDe9x9r7uPA/cA11f4/rNZd15NDUCiLpYiEjGVBP1y4KWi133hvOneYmaPm9kDZnbJWa5bc8taMphBn66OFZGIqSTorcQ8n/b6MeACd18HfB340VmsGxQ022RmvWbWOzg4WEG1qqs+leCyFa1871cvcHh4bN4/X0RkrlQS9H3AyqLXK4D+4gLuPuTuw+Hz+4E6M2urZN2i97jD3Xvcvae9vf0svkL1fOlDlzF0Msfn7n0S95L7IxGR804lQb8NuNDM1phZPXADsLm4gJl1mpmFz9eH73u4knUXkos6m/ns717ElqcO8k/b+2pdHRGRqpgx6N09B9wEPAQ8DfzA3XeZ2Y1mdmNY7MPATjN7HPgacIMHSq47F1+kWj75tjX82zVL+G///BQvHTlR6+qIiMyaLcQmip6eHu/t7a3Z5/e9coINf/MIa7tbuPuP3kwyUepUg4jIwmFm2929p9QyXRlbworFjXzh/Zfw6+eP8J2f7611dUREZkVBX8aHLl/O716yjL966FmeOTBU6+qIiJwzBX0ZZsZffuBSWhrq+Mz3H2csp4HDReT8pKA/g6WL0nzxQ5fy9P4hvvrwc7WujojIOVHQz+Cdb1jGxvUr+dutv2HbviO1ro6IyFlT0Ffgz96zlpWLG/mTH+xgWLcxFpHzjIK+Ak3pFF/5vXW8/MpJ/uKfF9yNN0VEzkhBX6Ge1Uu48e2v5fu9L/HwUwdrXR0RkYop6M/Cp9/1etZ2tfC5e5/Qjc9E5LyhoD8L9akEX/39N+nGZyJyXlHQn6XiG5/9o258JiLnAQX9Ofjk29bw5tcs4c914zMROQ8o6M9BImH81b9fhwH/+QePky+oCUdEFi4F/TlasbiRW99/Cb/ed4RvP6Ibn4nIwqWgn4UPXb6cDZd08tdbnuXp/brxmYgsTAr6WTAz/vKDkzc+26Ebn4nIgqSgn6UlTfV86cOX8syB43zl4WdrXR0RkdMo6KvgHRcHNz67Y+tefrZ7oNbVERE5hYK+Sv7sPWt5TVsTn/i7bXzu3ic5dmKi1lUSEQEU9FXTlE6x+aa38Ue/s4bvb3uRd37lX/nxjpd19ayI1JyCvoqa0ik+/561bL7pbSxvzfDH9+zgY3f+mhcOj9S6aiISYxUFvZltMLPdZrbHzG4usfwjZvZE+Pilma0rWrbPzJ40sx1m1lvNyi9Ub1ye5d7/dAVfeN9a/t+LR7nmq1u57ad7GM8Val01EYmhGYPezJLAbcC1wFpgo5mtnVbseeDt7n4Z8BfAHdOWX+3ub3L3nirU+byQTBifuGIND//JlVx9UQdffmg37/36I/RqlCoRmWeVHNGvB/a4+153HwfuAa4vLuDuv3T3V8KXjwIrqlvN81dXtoHbP/pbfPtjPYyM5fnw7f9XJ2tFZF5VEvTLgZeKXveF88r5JPBA0WsHtpjZdjPbdPZVjIZ3rV3Gls9cqZO1IjLvKgl6KzGvZDqZ2dUEQf+nRbOvcPfLCZp+PmVmV5ZZd5OZ9ZpZ7+DgYAXVOv+UOln78b/bxouHdQdMEZk7lQR9H7Cy6PUKoH96ITO7DPg2cL27H56c7+794XQAuI+gKeg07n6Hu/e4e097e3vl3+A8VHyy9rEXXuHdX/1XvvmzPUzkdbJWRKqvkqDfBlxoZmvMrB64AdhcXMDMVgH3Ah9192eL5jeZWfPkc+AaYGe1Kn8+m36y9ksP7uY9X3uEH/S+xO4Dx3XrYxGpmtRMBdw9Z2Y3AQ8BSeBOd99lZjeGy28HbgGWAt80M4Bc2MNmGXBfOC8FfM/dH5yTb3KemjxZ+3+eOsitm3fxX//pCQAa6pK8cXkLly5vZd3KLJcuz7J6aROJRKmWNBGR8mwhngzs6enx3t5YdLk/RaHg7D00zBN9x8LHUXb1DzEW9r9vzqS4dHmWy1a0ctmKLJetyLK8tYFwRyoiMWZm28t1YZ/xiF7mTyJhvK6jmdd1NPPBy4Meqrl8gWcPDvPky0d5vO8YT/Yd4zs/38tEPthBL22q59IVWS5bnuXSFa389urFtDbW1/JriMgCo6Bf4FLJBGu7W1jb3cLv/3YwbyyX55n9x3ni5WM88dJRnnz5GFufHaTgkDC4fNVirr64g3dc3MHFnc064heJOTXdRMSJ8Rw7Xx7i588N8pPdA+x8ORjxqjub4aqLO3jHRR1c8bo2GuqTNa6piMyFMzXdKOgj6uDQKD/bPcBPnhng588dYmQ8T30qwVtfu5R3XNzB1Rd1sHJJY62rKSJVoqCPubFcnm3Pv8JPnhngp7sHeP5QcDfNCzsWBaF/cQe/dcFi6pK6manI+UpBL6d4/tBIEPrPDPCr5w8zkXeaMymufH07ly3P0t6cfvWxKM3ixnp16xRZ4BT0UtbwWI6fP3eIn4ZH+wPHx04rk0oYbYtODf9TdgbhvI6WNI31Or8vUgvqXillLUqn2PDGTja8sROAkbEcg8fHGBweY2BojMHjowwOjwXzjo9xcGiUnS8f49DwGKUu3m1trGP10iZWL21kdVsTa9qagtdtTWQb6ub524kIKOhlmqZ0iqZ0itVtTWcsly84r5wYn9oBDBwfY+D4KH2vnGTfoRF+/fwRfrTj1FsiLWmqn9oBTIb/mqVNrG5rpDmjnYDIXFHQyzlJhs05bYvSvKGrdJnRiTwvHjnB84dG2HdohH2HR3j+0Ai/3HOYex97+ZSybYvqWb20iZVLGunMZujKZuhsydCVbaAzm2Fpk84TiJwrBb3MmUxdktcva+b1y5pPW3ZyPM8LR4IdwPOHTgTTwyNs23eEg0OjU1f+TqpPJliWTdPV0vDqjmBq2kBXNkPbojRJ7QxETqOgl5poqE9ycWcLF3e2nLasUHAOjYxx4Ngo+4+NFk1Psv/YKI/3HeXBXaOnjcGbTBiLG+tYlE6xKJMKpuk6miefh9Op1+G85nTd1LLWxjp1M5XIUdDLgpNIGB3NGTqaM1xWZlBKd+eVExPsP3aS/UdH2T8U7AheOTHB8GiO4bEcw6M5Xj56kuGxYN7x0Ry5Cm7/vLSpnvbmNMtaMnRMTlvSdDSn6QjntTenSad0lbGcHxT0cl4yM5Y01bOkqZ5LurMVrePujOUKUzuB4bEg/IfHcgyPTXB8NMeRkfHgxPLQKAPHx3jmwBCHhsdLjg+wuLGOZS0Z2pvTdDRnWNaSDpuTgqakrmyGJU31uteQ1JyCXmLDzMjUJcnUJWlblK54vXzBOTwy2d006GI6UDQdGBplz8Awg8fHTvuLIZ1KhKEfhn9r8Lx7cpptoKUhpZ2BzCkFvcgMkkVNSWcyeW5h/9HRoEkpPLfQfzR4/ujewxw8PnbaXweN9Uk6sxm6sw0sbqqnoS5Bpi5JQ12SdDjN1CXCafB8cofVcMo0QTJh5ApOruDk806uUCA/+Tqc5vKFU17nCwVy+eC1GaTrkqRTwWcUT9Op4DPSqSR1SdPO6TyioBepkuJzC+tWtpYsky84g8fH6J88tzC1QzhJ/9FRXj56ktGJPCcn8oxO5BmdWJjjCCeMU4J/cpquS1CfTFCXTFCfCh/JV6d1KaM+mQxf21SZyfKtDfWsWtLIqqWNusCuihT0IvMomTA6w66hrJq5fKHgjOcLnBzPM5rLB9OJAicn8oxN7RAKUzuHgjupRIJUwkgmjFQynCaMZDj/1XmJomXB/EIhuAne6ESBsVyesVzw3mO5AmMT015PPp8oMJoLpmO5POP5AhM55+jJCcZzBSbyBcZz4SNfYCJXYCycdybZhjouWNrIyiWNXLCkcWoHcMHSJjpbMjXpSuvunJzIMzyW48RYMB0Zy3FiPHgOTN0WpKM5zaL0wmiWU9CLLGCJhJFJBM0zUeMeNB1N7gQm8gUOj4zz4pETvHj4BC8eOcELR07wVP8QW3YdOOXaivpkghWLG4KdwNJwJ7CkkSVN9UzknYl8IXy8+jyXD3aauXD+eDivuOxoLs9IGN4jY3lGxnOvhnn4uoKOW1Ma6pJToT857WjJBPeLann1HlFLm+b2GhAFvYjUhJlRlzTqkgmawnPjHS0Z3tB1+rUV+YLTf/QkL4Xh/8LhE+HzER578RWOj+bOuR4JI2g6CpuPJm8DsiidZElTPSsXN9KUTobzgmVN9cmicika65MsSqcoOOG9okanTt4PhLcJefbgcX6x5xBDJeqaMFi6KM2apU384Ma3nPN3KUdBLyILXjJhrFwSNOO8tcTyoyfGeeHwCY6dnAjb+4OmqVOepxLUJYIdS10qaMaqSyaqfiR9UefpV4IXG53IF+0ARk/ZGcyVioLezDYA/wNIAt929/8+bbmFy68DTgCfcPfHKllXRGS2WhvraW2sr3U1KpKpS07ttObLjNd6m1kSuA24FlgLbDSztdOKXQtcGD42Ad86i3VFRGQOVXJTj/XAHnff6+7jwD3A9dPKXA981wOPAq1m1lXhuiIiMocqCfrlwEtFr/vCeZWUqWRdERGZQ5UEfakzFdM7GJUrU8m6wRuYbTKzXjPrHRwcrKBaIiJSiUqCvg9YWfR6BdBfYZlK1gXA3e9w9x5372lvb6+gWiIiUolKgn4bcKGZrTGzeuAGYPO0MpuBj1ngzcAxd99f4boiIjKHZuxe6e45M7sJeIigi+Sd7r7LzG4Ml98O3E/QtXIPQffKPzjTunPyTUREpCRzP4vreedJT0+P9/b21roaIiLnDTPb7u49JZctxKA3s0HghVrXo4w24FCtK3EGqt/sqH6zo/rNzmzqd4G7lzzBuSCDfiEzs95ye82FQPWbHdVvdlS/2Zmr+mkUZBGRiFPQi4hEnIL+7N1R6wrMQPWbHdVvdlS/2ZmT+qmNXkQk4nRELyIScQr6EsxspZn91MyeNrNdZvbHJcpcZWbHzGxH+Lhlnuu4z8yeDD/7tIsOwquUv2Zme8zsCTO7fB7rdlHRdtlhZkNm9ulpZeZ1+5nZnWY2YGY7i+YtMbOHzey5cLq4zLobzGx3uC1vnsf6fdnMngn//e4zs9Yy657xtzCH9fuCmb1c9G94XZl1a7X9vl9Ut31mtqPMuvOx/Upmyrz9Bt1dj2kPoAu4PHzeDDwLrJ1W5irgf9ewjvuAtjMsvw54gODGcm8GflWjeiaBAwR9fGu2/YArgcuBnUXzvgTcHD6/Gfhimfr/BngNUA88Pv23MIf1uwZIhc+/WKp+lfwW5rB+XwD+SwX//jXZftOW/zVwSw23X8lMma/foI7oS3D3/R6OkOXux4GnOf9ur1xujID59k7gN+5e0wvg3H0rcGTa7OuBvw+f/z3w70qsOi9jKpSqn7tvcffJAUYfJbgpYE2U2X6VqNn2m2RmBvwecHe1P7dSZ8iUefkNKuhnYGargX8D/KrE4reY2eNm9oCZXTK/NcOBLWa23cw2lVi+UMYCuIHy/8Fquf0Alnlw8z3CaUeJMgtlO/4Hgr/QSpnptzCXbgqblu4s0+ywELbf7wAH3f25MsvndftNy5R5+Q0q6M/AzBYBPwQ+7e5D0xY/RtAcsQ74OvCjea7eFe5+OcEwjZ8ysyunLa94LIC5YsEdS98P/GOJxbXefpVaCNvx80AO+IcyRWb6LcyVbwGvBd4E7CdoHpmu5tsP2MiZj+bnbfvNkCllVysx76y2oYK+DDOrI/gH+Qd3v3f6cncfcvfh8Pn9QJ2Ztc1X/dy9P5wOAPcR/HlXrOKxAObQtcBj7n5w+oJab7/QwcnmrHA6UKJMTbejmX0ceC/wEQ8bbKer4LcwJ9z9oLvn3b0A/M8yn1vr7ZcCPgh8v1yZ+dp+ZTJlXn6DCvoSwja97wBPu/tXypTpDMthZusJtuXheapfk5k1Tz4nOGm3c1qxcmMEzKeyR1K13H5FNgMfD59/HPhxiTI1G1PBzDYAfwq8391PlClTyW9hrupXfM7nA2U+t9ZjUrwLeMbd+0otnK/td4ZMmZ/f4FyeaT5fH8DbCP40egLYET6uA24EbgzL3ATsIjgD/ijw1nms32vCz308rMPnw/nF9TPgNoKz9U8CPfO8DRsJgjtbNK9m249gh7MfmCA4QvoksBT4F+C5cLokLNsN3F+07nUEvSR+M7mt56l+ewjaZid/g7dPr1+538I81e9/hb+tJwiCp2shbb9w/l2Tv7misrXYfuUyZV5+g7oyVkQk4tR0IyIScQp6EZGIU9CLiEScgl5EJOIU9CIiEaegFxGJOAW9iEjEKehFRCLu/wOY5Tdv9cVBjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(dataloader)\n",
    "curr_lr = learning_rate\n",
    "loss_vals=  []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss= []\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        \n",
    "        images=sample_batched['image'];\n",
    "        labels=sample_batched['labels']\n",
    "        images = images.unsqueeze(1)\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        labels= torch.reshape(labels, (-1,))\n",
    "        labels = labels.to(dtype=torch.float32) \n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i_batch+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i_batch+1, total_step, loss.item()))\n",
    "    loss_vals.append(sum(epoch_loss)/len(epoch_loss))\n",
    "    \n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "        \n",
    "my_plot(np.linspace(1, num_epochs, num_epochs).astype(int), loss_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the train images: 93.90579656224422 %\n"
     ]
    }
   ],
   "source": [
    "#Training set accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        images=sample_batched['image'];\n",
    "        labels=sample_batched['labels']\n",
    "        images = images.unsqueeze(1)\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        labels= torch.reshape(labels, (-1,))\n",
    "        labels = labels.to(dtype=torch.float32) \n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the train images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 90.41381363501041 %\n"
     ]
    }
   ],
   "source": [
    "#Test set accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i_batch, sample_batched in enumerate(test_loader):\n",
    "        images=sample_batched['image'];\n",
    "        labels=sample_batched['labels']\n",
    "        images = images.unsqueeze(1)\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        labels= torch.reshape(labels, (-1,))\n",
    "        labels = labels.to(dtype=torch.float32) \n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
