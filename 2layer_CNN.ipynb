{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 10\n",
    "num_classes = 28\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataset(Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        labels = self.landmarks_frame\n",
    "        labels = np.array(labels)\n",
    "        labels = labels.astype('float')\n",
    "        \n",
    "        A=[1]\n",
    "        labels = np.vstack([A, labels])\n",
    "        \n",
    "        \n",
    "        #print(labels.shape[0])\n",
    "        #print(labels)\n",
    "        get_label=labels[idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        labels=get_label.astype('float');\n",
    "        \n",
    "        \n",
    "        str1= 'id_'+str(idx+1)+'_label_'+str(int(labels))+'.png';\n",
    "        labels=labels-1;\n",
    "        img_name = os.path.join(self.root_dir, str1)\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        sample = {'image': image, 'labels': labels}\n",
    "        #print(\"idx\", \" \", idx, \" \", \"labels\", labels)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, labels = sample['image'], sample['labels']\n",
    "        #print(type(image))\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        #image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'labels': torch.from_numpy(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = GetDataset(csv_file='arabic_letters/csvTrainLabel 13440x1.csv',\n",
    "                                           root_dir='arabic_letters/Train Images 13440x32x32/train',\n",
    "                                           transform=transforms.Compose([\n",
    "                                              \n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=28):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(2*32*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #print(out.size)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot(epochs, loss):\n",
    "    plt.plot(epochs, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/135], Loss: 1.3553\n",
      "tensor(1.3553, grad_fn=<NllLossBackward>)\n",
      "Epoch [2/10], Step [100/135], Loss: 0.5089\n",
      "tensor(0.5089, grad_fn=<NllLossBackward>)\n",
      "Epoch [3/10], Step [100/135], Loss: 0.3075\n",
      "tensor(0.3075, grad_fn=<NllLossBackward>)\n",
      "Epoch [4/10], Step [100/135], Loss: 0.3730\n",
      "tensor(0.3730, grad_fn=<NllLossBackward>)\n",
      "Epoch [5/10], Step [100/135], Loss: 0.2124\n",
      "tensor(0.2124, grad_fn=<NllLossBackward>)\n",
      "Epoch [6/10], Step [100/135], Loss: 0.1947\n",
      "tensor(0.1947, grad_fn=<NllLossBackward>)\n",
      "Epoch [7/10], Step [100/135], Loss: 0.3107\n",
      "tensor(0.3107, grad_fn=<NllLossBackward>)\n",
      "Epoch [8/10], Step [100/135], Loss: 0.2061\n",
      "tensor(0.2061, grad_fn=<NllLossBackward>)\n",
      "Epoch [9/10], Step [100/135], Loss: 0.1799\n",
      "tensor(0.1799, grad_fn=<NllLossBackward>)\n",
      "Epoch [10/10], Step [100/135], Loss: 0.0746\n",
      "tensor(0.0746, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(dataloader)\n",
    "loss_vals=  []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss= []\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        \n",
    "        images=sample_batched['image'];\n",
    "        labels=sample_batched['labels']\n",
    "        images = images.unsqueeze(1)\n",
    "        images = images.type(torch.FloatTensor)\n",
    "      \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "                    \n",
    "        labels= torch.reshape(labels, (-1,))\n",
    "        labels = labels.to(dtype=torch.float32) \n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if (i_batch+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i_batch+1, total_step, loss.item()))\n",
    "            print(loss)\n",
    "            \n",
    "    loss_vals.append(sum(epoch_loss)/len(epoch_loss))\n",
    "    \n",
    "my_plot(np.linspace(1, num_epochs, num_epochs).astype(int), loss_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GetDataset(csv_file='arabic_letters/csvTestLabel 3360x1.csv',\n",
    "                                           root_dir='arabic_letters/Test Images 3360x32x32/test',\n",
    "                                           transform=transforms.Compose([\n",
    "                                              \n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 3360 test images: 97.57422427263933 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        \n",
    "        images=sample_batched['image'];\n",
    "        labels=sample_batched['labels']\n",
    "        images = images.unsqueeze(1)\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        \n",
    "        labels= torch.reshape(labels, (-1,))\n",
    "        labels = labels.to(dtype=torch.float32) \n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        \n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 3360 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
