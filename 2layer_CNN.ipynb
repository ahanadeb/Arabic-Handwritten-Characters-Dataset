{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 10\n",
    "num_classes = 28\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataset(Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        labels = self.landmarks_frame\n",
    "        labels = np.array(labels)\n",
    "        labels = labels.astype('float')\n",
    "        \n",
    "        A=[1]\n",
    "        labels = np.vstack([A, labels])\n",
    "        \n",
    "        \n",
    "        #print(labels.shape[0])\n",
    "        #print(labels)\n",
    "        get_label=labels[idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        labels=get_label.astype('float');\n",
    "        \n",
    "        \n",
    "        str1= 'id_'+str(idx+1)+'_label_'+str(int(labels))+'.png';\n",
    "        labels=labels-1;\n",
    "        img_name = os.path.join(self.root_dir, str1)\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        sample = {'image': image, 'labels': labels}\n",
    "        #print(\"idx\", \" \", idx, \" \", \"labels\", labels)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, labels = sample['image'], sample['labels']\n",
    "        #print(type(image))\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        #image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'labels': torch.from_numpy(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = GetDataset(csv_file='arabic_letters/csvTrainLabel 13440x1.csv',\n",
    "                                           root_dir='arabic_letters/Train Images 13440x32x32/train',\n",
    "                                           transform=transforms.Compose([\n",
    "                                              \n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)      \n",
    "\n",
    "\n",
    "test_dataset = GetDataset(csv_file='arabic_letters/csvTestLabel 3360x1.csv',\n",
    "                                           root_dir='arabic_letters/Test Images 3360x32x32/test',\n",
    "                                           transform=transforms.Compose([\n",
    "                                              \n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=28):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(2*32*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #print(out.size)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_plot(epochs, loss):\n",
    "    plt.plot(epochs, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/135], Loss: 1.1930\n",
      "tensor(1.1930, grad_fn=<NllLossBackward>)\n",
      "Epoch [2/10], Step [100/135], Loss: 0.6168\n",
      "tensor(0.6168, grad_fn=<NllLossBackward>)\n",
      "Epoch [3/10], Step [100/135], Loss: 0.3101\n",
      "tensor(0.3101, grad_fn=<NllLossBackward>)\n",
      "Epoch [4/10], Step [100/135], Loss: 0.4432\n",
      "tensor(0.4432, grad_fn=<NllLossBackward>)\n",
      "Epoch [5/10], Step [100/135], Loss: 0.3929\n",
      "tensor(0.3929, grad_fn=<NllLossBackward>)\n",
      "Epoch [6/10], Step [100/135], Loss: 0.3510\n",
      "tensor(0.3510, grad_fn=<NllLossBackward>)\n",
      "Epoch [7/10], Step [100/135], Loss: 0.2301\n",
      "tensor(0.2301, grad_fn=<NllLossBackward>)\n",
      "Epoch [8/10], Step [100/135], Loss: 0.1248\n",
      "tensor(0.1248, grad_fn=<NllLossBackward>)\n",
      "Epoch [9/10], Step [100/135], Loss: 0.0862\n",
      "tensor(0.0862, grad_fn=<NllLossBackward>)\n",
      "Epoch [10/10], Step [100/135], Loss: 0.0548\n",
      "tensor(0.0548, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(dataloader)\n",
    "loss_vals=  []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss= []\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        \n",
    "        images=sample_batched['image'];\n",
    "        labels=sample_batched['labels']\n",
    "        images = images.unsqueeze(1)\n",
    "        images = images.type(torch.FloatTensor)\n",
    "      \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "                    \n",
    "        labels= torch.reshape(labels, (-1,))\n",
    "        labels = labels.to(dtype=torch.float32) \n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if (i_batch+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i_batch+1, total_step, loss.item()))\n",
    "            print(loss)\n",
    "            \n",
    "    loss_vals.append(sum(epoch_loss)/len(epoch_loss))\n",
    "    \n",
    "my_plot(np.linspace(1, num_epochs, num_epochs).astype(int), loss_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of the model on the 13440 train images: 99.15916362824615 %\n"
     ]
    }
   ],
   "source": [
    "#Train set accuracy\n",
    "\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        \n",
    "        images=sample_batched['image'];\n",
    "        labels=sample_batched['labels']\n",
    "        images = images.unsqueeze(1)\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        \n",
    "        labels= torch.reshape(labels, (-1,))\n",
    "        labels = labels.to(dtype=torch.float32) \n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        \n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Training Accuracy of the model on the 13440 train images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 3360 test images: 89.6100029770765 %\n"
     ]
    }
   ],
   "source": [
    "#Test set accuracy\n",
    "\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i_batch, sample_batched in enumerate(test_loader):\n",
    "        \n",
    "        images=sample_batched['image'];\n",
    "        labels=sample_batched['labels']\n",
    "        images = images.unsqueeze(1)\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        \n",
    "        labels= torch.reshape(labels, (-1,))\n",
    "        labels = labels.to(dtype=torch.float32) \n",
    "        labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        \n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 3360 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
